{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information, Entropy and statistical mechanics\n",
    "The quantity $H=-\\avg{\\log p}$ is called Entropy or Information, quite interchangeably. You probably know from your prior physics education that Entropy is a fundamental concept in thermodynamic and statistical mechanics, and is defined in the same way as we did here, except for the Boltzmann factor $k_B$. There is a deep connection between the thermodynamic Entropy and Shannon Information.\n",
    "\n",
    "In statistical mechanics, the *micro-canonical* ensemble is a probability measure over phase space which vanishes outside of a thin energy shell, and is constant within this shell. This distribution is the distribution that maximizes the information (entropy) given our knowledge of the system: in our case - energy conservation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{admonition} Active learning\n",
    ":class: tip\n",
    "1. Show that for a discrete variable $X\\in\\{x_1\\dots x_N\\}$ the probability distribution that maximizes the entropy is the constant  one $p(x_i)=\\frac{1}{N}$.\n",
    "1. Show that for a continuous variable $X\\in[x_0, x_1]$ the probability distribution that maximizes the continuous entropy $-\\int_{x_0}^{x_1} p(x) \\log p(x) \\, dx$ is the constant one $p(x)=\\frac{1}{x_1-x_0}$.\n",
    "1. What is the continuous distribution that maximizes the entropy of a continuous variable $X\\in (-\\infty, \\infty)$, given that we know $\\avg{X}=\\mu$ and $\\avg{X^2}=\\sigma$?\n",
    "1. What is the distribution that maximizes the entropy of a discrete random variable  $\\epsilon \\in \\{\\epsilon_1, \\epsilon_2, \\dots\\}$, given that we know $\\avg{\\epsilon}=E$?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *canonical ensemble*, the the probability of observing an energy $E$ at temperature $T=1/\\beta$ is given by summing all microstates $\\sigma$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(E;T) &= \\sum_{\\sigma} \\frac{e^{-\\beta E(\\sigma)}\\delta(H(\\sigma)-E)}{Z}\n",
    "    = \\frac{e^{-\\beta E}\\Omega(E)}{Z} \\\\\n",
    "    &= \\frac{e^{-\\beta (E-TS)}}{Z} \n",
    "    = \\frac{e^{-\\beta F}}{Z} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Inverting, we find that $-\\beta F(E) = \\log(p(E))$. That is, $-\\beta F$ is the log-probability (or log-likelihood) of observing the energy $E$. \n",
    "More generally, if we have an observable $X$ that affects the energy - say the volume of a gas, the conformation of a protein, the magnetization in a spin system - we can write an associated free energy defined by $-\\beta F(X)=\\log(p(X))$. \n",
    "\n",
    "In other words, the relative frequency of observing $x_1$ vs $x_2$ is given by $\\frac{p(x1)}{p(x_2)}=e^{-\\beta \\Delta F}$. If the free energy difference is comparable to $T$, you'd expect to measure both $x_1$ and $x_2$ at some finite ratio. If the difference is large, you'd only see the lower one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{admonition} Active learning\n",
    ":class: tip\n",
    "What's the free energy difference between the folded and unfolded protein of [our protein from before](protein-example) ath room temperature $T=300$K?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
