# Course: Data science for scientists
Lecturer: Yohai Bar-Sinai

Audience: Graduate students, 3rd year undergraduates. Prerequisites: calculus, linear algebra, probability

1. Refresher on probability
    1. Basic probability facts
        1. Definitions
        1. Central limit theorem, Law of Large Numbers
        1. Information and Entropy
    1. Important probability distributions
        1. Gaussian distribution
        1. Binomial distribution
        1. Poisson distribution
        1. ...etc...

1. Parameter estimation, Bayesian analysis
    1. Bayes' theorem
    1. Maximum likelihood estimation
    1. Method of moments
    1. Confidence intervals
    1. Kramer-Rao bound

1. Hypothesis testing

1. Monte-Carlo methods
    1. Sampling from distributions
    1. MC integration
    1. MCMC inference, Gibbs sampling

1. Linear models
    1. Linear regression
        1. Maximum likelihood (least squares)
        1. Ridge regression
        1. Logistic regression
    1. Sparse linear models
    1. Generalized linear models
        1. Kernel trick
        1. Support Vector Machines

1. Dimensionality reduction
    1. PCA, Kernel PCA
    1. ICA
    1. Diffusion maps

1. Noise and denoising
    1. Measurement errors and propagation analysis
    1. SVD denoising
    1. State-space models (Kalman Filters)
    1. Dynamic mode decomposition

1. Statistical learning
    1. Bias-variance tradeoff
    1. Mixture models and EM algorithm
    1. Bootstrap
    1. k-nearest neighbors
    1. k-means

1. Deep neural networks
    1. Introduction and setting
        1. Backpropagation
        1. Activations
        1. Losses
        1. Optimizers, regularization
        1. Train-validation-test split
    1. Common architectures
        1. FCNs
        1. Convolutional neural networks